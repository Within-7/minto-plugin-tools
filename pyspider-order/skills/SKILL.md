---
name: pyspider-order
description: "Manage PySpider web scraping tasks via Feishu API. Use when users request: (1) Social media scraping (Reddit, Instagram, TikTok, Twitter, Facebook), (2) E-commerce data (Amazon, å–å®¶ç²¾çµ), (3) SEO data (SEMrush). Maps natural language to PySpider projects with validation, status checks, and notifications."
---

# PySpider Task Management

**Tool Operation Skill** - Low freedom, precise steps for external system integration.

## Purpose

Enable analysts to order PySpider web scraping tasks through natural language. Integrates Feishu API and PySpider dispatcher with validation and error handling.

## Trigger

Use when user requests:
- "æŠ“å– Reddit/Instagram/TikTok/Twitter/Facebook æ•°æ®"
- "Amazon è¯„è®º/å–å®¶ç²¾çµæ•°æ®"
- "SEMrush å¤–é“¾æ•°æ®"
- Any web scraping from social media, e-commerce, or SEO tools

## Workflow

**CRITICAL**: Execute step-by-step. NEVER skip steps or combine them. Each step MUST complete successfully before proceeding.

### Step 1: Parse Request (DO NOT SKIP)

Extract from user's natural language:
- Platform (åª’ä½“ç±»å‹) - e.g., "Reddit å…³é”®è¯ä¸‹çš„å¸–å­"
- Keywords (å…³é”®è¯) - e.g., ["AI", "machine learning"]

**If unclear**: Show available platforms:
```python
from scripts.crawlers import format_crawlers_for_display
available = format_crawlers_for_display()
print(available)
```

**Ask user**: "è¯·é€‰æ‹©å¹³å°ï¼š\n1. Reddit (å…³é”®è¯)\n2. Instagram (æ ‡ç­¾)\n3. TikTok (å…³é”®è¯)\n4. ..."

**STOP HERE until user provides platform + keywords**

### Step 2: Get Crawler Config (CRITICAL - DO NOT SKIP)

```python
from scripts.crawlers import get_crawler_info

info = get_crawler_info(media_type)
if not info:
    available = format_crawlers_for_display()
    return f"âŒ ä¸æ”¯æŒçš„å¹³å°: {media_type}\n\n{available}"

project = info["project"]
field = info["field"]
validation = info.get("validation")  # e.g., "must start with https://www.facebook.com/"
```

**DO NOT PROCEED if config not found**

### Step 3: Validate Parameters (CRITICAL - DO NOT SKIP)

**Facebook Ads**: URL must start with `https://www.facebook.com/`
- Check: `keyword.startswith('https://www.facebook.com/')`
- If invalid: Return error with format requirement

**Other platforms**: Basic validation (not empty, length < 500)

**DO NOT PROCEED if validation fails**

### Step 4: Check PySpider Project Status (CRITICAL - DO NOT SKIP)

```python
from scripts.check_project_status import check_project_status

status = check_project_status(project)
if not status['exists']:
    return f"âŒ PySpideré¡¹ç›®ä¸å­˜åœ¨: {project}\nè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆç¡®è®¤é¡¹ç›®é…ç½®"

if not status['can_run']:
    return f"âŒ PySpideré¡¹ç›®çŠ¶æ€å¼‚å¸¸: {status['status']}\né¡¹ç›®å¿…é¡»å¤„äº RUNNING æˆ– DEBUG çŠ¶æ€æ‰èƒ½æ‰§è¡Œ\nè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆå¤„ç†"
```

**DO NOT PROCEED if project not RUNNING or DEBUG**

### Step 5: Create Feishu Record (DO NOT PROCEED if environment variables not set)

**First check environment variables**:
```python
import os
if not os.getenv('MONGODB_URL'):
    return "âŒ ç¯å¢ƒå˜é‡ MONGODB_URL æœªé…ç½®\nè¯·è®¾ç½®: export MONGODB_URL=\"mongodb://user:pass@host:port\""

if not os.getenv('FEISHU_API_URL'):
    return "âŒ ç‹è¯•é£ä¹¦APIè¿æ¥å¤±è´¥: $FEISHU_API_URL æœªé…ç½®\nè¯·å‚è€ƒ README.md é…ç½®ç¯å¢ƒå˜é‡"
```

**Then create record**:
```python
from scripts.feishu_client import FeishuClient
from scripts.order import create_order

result = create_order(media_type, keywords, task_user='minto')

if not result['success']:
    return f"âŒ ä¸‹å•å¤±è´¥: {result['message']}"

return f"âœ… ä¸‹å•æˆåŠŸï¼\n{result['message']}"
```

**DO NOT PROCEED if Feishu API fails**

## Supported Platforms

**Social Media** (field type):
- Reddit å…³é”®è¯ä¸‹çš„å¸–å­ (keyword)
- Instagram æ ‡ç­¾ä¸‹çš„å¸–å­ (tags)
- TikTok æ ‡ç­¾ä¸‹çš„å¸–å­ (keyword)
- Twitter å…³é”®è¯ä¸‹çš„å¸–å­ (keyword)
- Facebook Ads ä¸»é¡µä¸‹çš„å¹¿å‘Š (url, must start with https://www.facebook.com/)
- Youtube å…³é”®è¯ä¸‹çš„è§†é¢‘ (keyword)
- Pinterest å…³é”®è¯çš„æ‰€æœ‰å¸–å­ (keyword)

**E-commerce**:
- Amazonåˆ—è¡¨æ‰€æœ‰äº§å“åŠè¯„è®º (keywords)
- å–å®¶ç²¾çµçš„å“ç‰Œé”€å”®é¢æ•°æ® (brand)
- å–å®¶ç²¾çµçš„å–å®¶é”€å”®é¢æ•°æ® (seller)
- å–å®¶ç²¾çµçš„å…³é”®è¯é”€å”®é¢æ•°æ® (keyword)

**SEO Tools**:
- semrushä¸­çš„å¤–é“¾æ•°æ®æŠ“å– (keyword)

## NEVER

- âŒ Skip parameter validation
- âŒ Execute without checking PySpider project status (must be RUNNING or DEBUG)
- âŒ Use command line interface (`python run.py...`) - Use Python imports instead
- âŒ Create orders for incomplete crawlers (check crawler info exists first)

## Error Handling

- **Validation errors** â†’ Show specific error with format requirements
- **Project not found** â†’ Notify via Feishu, mark as "ç­‰å¾…æ‰‹åŠ¨å¤„ç†"
- **Project status error** (not RUNNING/DEBUG) â†’ Notify via Feishu, mark as "ç­‰å¾…æ‰‹åŠ¨å¤„ç†"
- **API failures** â†’ Notify via Feishu, mark as "ç­‰å¾…æ‰‹åŠ¨å¤„ç†"

## Example Interactions

### Example 1: Successful Request

**User**: "å¸®æˆ‘æŠ“å–Redditä¸Šå…³äºAIçš„å¸–å­"

**AI Execution**:
```
Step 1: Parse â†’ platform="Reddit å…³é”®è¯ä¸‹çš„å¸–å­", keywords=["AI"]
Step 2: Get config â†’ project="ScrapingRedditByKeyword_api", field="keyword"
Step 3: Validate â†’ âœ“ (no special validation needed)
Step 4: Check status â†’ âœ“ RUNNING
Step 5: Create record â†’ record_id="rec123"
Step 6: Send task â†’ âœ“ 1/1 keywords sent
Step 7: Update & notify â†’ âœ“
```

**Response**: "âœ… ä¸‹å•æˆåŠŸï¼\nä»»åŠ¡ID: xxx-xxx-xxx\né£ä¹¦è®°å½•ID: rec123\né¡¹ç›®: ScrapingRedditByKeyword_api\næˆåŠŸå‘é€ 1/1 ä¸ªå…³é”®è¯"

---

### Example 2: Validation Failure

**User**: "æŠ“å–Facebookå¹¿å‘Šï¼Œhttps://example.com/ads"

**AI Execution**:
```
Step 1: Parse â†’ platform="Facebook Ads ä¸»é¡µä¸‹çš„å¹¿å‘Š", keywords=["https://example.com/ads"]
Step 2: Get config â†’ field="url", validation="must start with https://www.facebook.com/"
Step 3: Validate â†’ âœ— URL format error (doesn't start with https://www.facebook.com/)
```

**Response**: "âŒ Facebook URLæ ¼å¼é”™è¯¯: https://example.com/ads\n\næ ¼å¼è¦æ±‚ï¼šå¿…é¡»ä»¥ https://www.facebook.com/ å¼€å¤´\n\nè¯·æä¾›æ­£ç¡®çš„Facebook Ads URL"

---

### Example 3: Platform Not Supported

**User**: "å¸®æˆ‘æŠ“å–LinkedInæ•°æ®"

**AI Execution**:
```
Step 1: Parse â†’ platform="LinkedIn", keywords=unknown
Step 2: Get config â†’ get_crawler_info("LinkedIn") = None
```

**Response**: "âŒ ä¸æ”¯æŒçš„å¹³å°: LinkedIn\n\næ”¯æŒçš„å¹³å°ï¼š\nğŸ“± ç¤¾äº¤åª’ä½“ï¼š\n- Reddit å…³é”®è¯ä¸‹çš„å¸–å­ (keyword)\n- Instagram æ ‡ç­¾ä¸‹çš„å¸–å­ (tags)\n- TikTok æ ‡ç­¾ä¸‹çš„å¸–å­ (keyword)\n- ..."

---

### Example 4: Project Status Error

**User**: "æŠ“å–SEMrushå¤–é“¾æ•°æ®ï¼Œbaidu.com"

**AI Execution**:
```
Step 1: Parse â†’ platform="semrushä¸­çš„å¤–é“¾æ•°æ®æŠ“å–", keywords=["baidu.com"]
Step 2: Get config â†’ project="BackLink", field="keyword"
Step 3: Validate â†’ âœ“
Step 4: Check status â†’ âœ— status="STOP" (not RUNNING/DEBUG)
```

**Response**: "âŒ PySpideré¡¹ç›®çŠ¶æ€å¼‚å¸¸: STOP\n\né¡¹ç›®: BackLink\nå½“å‰çŠ¶æ€: STOP\nè¦æ±‚çŠ¶æ€: RUNNING æˆ– DEBUG\n\nè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆå¤„ç†"

## Environment Variables Required

```bash
MONGODB_URL              # MongoDB connection
FEISHU_API_URL          # Feishu API server
FEISHU_TABLE_TOKEN       # Feishu table token
FEISHU_TABLE_ID          # Feishu table ID
FEISHU_WEBHOOK           # Feishu webhook URL
PYSPIDER_BASE_URL        # PySpider server URL
PYSPIDER_SESSION_COOKIE   # PySpider session cookie
```

## Implementation Notes

- All scripts in `scripts/` directory
- Main function: `scripts.order.create_order(media_type, keywords, task_user)`
- Crawler config: `scripts.crawlers.get_crawler_info(name)`
- No CLI commands needed - Minto calls Python functions directly

## Troubleshooting

### If execution fails, check in order:

**1. Environment Variables (CRITICAL)**
```bash
# Check all required variables are set
echo $MONGODB_URL
echo $FEISHU_API_URL
echo $FEISHU_TABLE_TOKEN
echo $FEISHU_TABLE_ID
echo $FEISHU_WEBHOOK
echo $PYSPIDER_BASE_URL
echo $PYSPIDER_SESSION_COOKIE
```

If any are empty â†’ "âŒ ç¯å¢ƒå˜é‡æœªé…ç½®ï¼Œè¯·è®¾ç½®åé‡è¯•"

**2. Feishu API Connection**
```bash
# Test Feishu API
curl -s "$FEISHU_API_URL/api/query_scraping_form_data" | jq .
```

If connection fails â†’ "âŒ æ— æ³•è¿æ¥é£ä¹¦API: $FEISHU_API_URL\nè¯·æ£€æŸ¥ç½‘ç»œå’ŒAPIåœ°å€"

**3. PySpider Project Status**
```python
from scripts.check_project_status import check_project_status
status = check_project_status('BackLink')
print(f"Exists: {status['exists']}, Status: {status['status']}, Can Run: {status['can_run']}")
```

**4. MongoDB Connection**
```python
from pymongo import MongoClient
import os
client = MongoClient(os.getenv('MONGODB_URL'))
print('âœ“ MongoDB connected' if client else 'âŒ MongoDB connection failed')
```

### Common Issues

| Error | Cause | Solution |
|------|-------|----------|
| ModuleNotFoundError: No module named 'scripts' | Wrong working directory | cd to project root containing scripts/ directory |
| âŒ ç¯å¢ƒå˜é‡æœªé…ç½® | .env not loaded | Set environment variables from README.md |
| âŒ æ— æ³•è¿æ¥é£ä¹¦API | Wrong API URL or network | Check $FEISHU_API_URL and network connection |
| âŒ PySpideré¡¹ç›®ä¸å­˜åœ¨ | Project not in MongoDB | Contact çˆ¬è™«å·¥ç¨‹å¸ˆ to add project |
| âŒ PySpideré¡¹ç›®çŠ¶æ€å¼‚å¸¸ | Project stopped/error | Contact çˆ¬è™«å·¥ç¨‹å¸ˆ to start project |
