---
name: pyspider-order
description: "Manage PySpider web scraping tasks through natural language. Use when analysts request: (1) Scraping social media data (Reddit, Instagram, TikTok, Twitter, Facebook), (2) Keyword-based content collection, (3) E-commerce data scraping (Amazon, SellerSprite), (4) SEO data extraction (SEMrush). Maps natural language requests to PySpider projects via Feishu API integration."
---

# PySpider Order

Enable analysts to order web scraping tasks through natural language.

## Core Philosophy

**This is a Tool Operation skill** - Low freedom, precise steps.

The plugin manages complex external systems (Feishu, PySpider). Every decision matters.
Goal: Transform natural language â†’ structured API calls with 100% reliability.

## Mental Model: How to Approach User Requests

### Decision Tree

```
ç”¨æˆ·è¯·æ±‚
    â†“
èƒ½å¦ç›´æ¥è§£æå‡º"å¹³å°+å…³é”®è¯"ï¼Ÿ
    â†“ Yes
    ç›´æ¥æ ¡éªŒå‚æ•° â†’ ç¡®è®¤ â†’ æ‰§è¡Œ
    â†“ No
    ç”¨æˆ·éœ€æ±‚æ¨¡ç³Šï¼Ÿ
    â†“ Yes
    å±•ç¤ºåˆ†ç±»é€‰é¡¹ â†’ è¯¢é—®å¹³å°å’Œå…³é”®è¯
    â†“ No
    ç”¨æˆ·è¯´äº†å¹³å°ä½†æ²¡è¯´å…³é”®è¯ï¼Ÿ
    â†“ Yes
    ç¡®è®¤å¹³å° â†’ è¯¢é—®å…³é”®è¯
```

### Key Decision Principles

**1. Parse First, Ask Second**
- Always try to extract info from user's initial request
- Don't ask unless necessary
- Example: "æŠ“Redditä¸Šå…³äºAIçš„å¸–å­" â†’ ç›´æ¥æå–ï¼Œä¸è¦é—®"å“ªä¸ªå¹³å°ï¼Ÿ"

**2. Guide, Don't Overwhelm**
- If unclear, show categorized options (ç¤¾äº¤åª’ä½“/ç”µå•†/SEOå·¥å…·)
- Don't show all 20+ crawlers at once
- Let user drive the conversation

**3. Validate Before Confirming**
- Never skip validation
- Show all collected info before executing
- One final confirmation with all details

**4. Multi-Keyword Awareness**
- Support natural separators: é€—å·ã€é¡¿å·ã€æ¢è¡Œ
- "AIã€machine learningã€crypto" â†’ 3 keywords
- "AI, machine learning" â†’ 2 keywords
- Parse automatically, don't ask user to format

## NEVER Do These (Anti-Patterns)

**Interaction Anti-Patterns:**
- âŒ Don't use "å…¶ä»–" (Other) option in AskUserQuestion - causes deadlock
- âŒ Don't ask all questions at once - one question at a time
- âŒ Don't force users to pick from dropdowns - let them type freely
- âŒ Don't treat comma-separated keywords as single keyword

**Technical Anti-Patterns:**
- âŒ NEVER write custom test scripts - always use `scripts/`
- âŒ NEVER skip validation - always use `run.py validate`
- âŒ NEVER access MongoDB directly - only through scripts
- âŒ NEVER execute without user confirmation

## Interaction Examples

### 1. List Available Crawlers

When user request is unclear or wants to see options:

**Use:** `run.py list`

```bash
python run.py list
```

This displays all 20+ crawlers organized by category.

### 2. Order Crawl Task

Parse natural language requests and create scraping tasks:

**MANDATORY Workflow:**
1. Parse user's natural language request â†’ Extract media type + keywords
2. If unclear, use `run.py list` to show options
3. Intelligently parse multi-keywords (support: é€—å·ã€é¡¿å·ã€æ¢è¡Œ)
4. Validate parameters using `run.py validate`
5. Show confirmation with all collected info
6. Only after user confirms, use `run.py order` to execute

**Execute order:**
```bash
python run.py order "Reddit å…³é”®è¯ä¸‹çš„å¸–å­" "AI, machine learning" "ou_xxx"
```

## Pre-built Scripts (NEVER rewrite these)

**Query & Display:**
- `scripts/list_all_crawlers.py` - List all available crawlers with examples

**Validation:**
- `scripts/validate_params.py` - Strict parameter validation (URL, keywords, multi-keyword parsing)

**Execution:**
- `scripts/create_crawl_order.py` - Complete order workflow (validation â†’ status check â†’ create â†’ dispatch)
- `scripts/check_project_status.py` - Check PySpider project status (internal use)
- `scripts/feishu_client.py` - Feishu API client
- `scripts/pyspider_dispatcher.py` - PySpider dispatcher client

## Parameter Validation

**Always use `run.py validate`:**

```bash
python run.py validate "Reddit å…³é”®è¯ä¸‹çš„å¸–å­" "AI, machine learning"
```

**Validation rules:**
- Multi-keyword support: "AI, machine learning" â†’ ["AI", "machine learning"]
- URL format (Facebook Ads must start with https://www.facebook.com/)
- Keyword length (max 500 chars each)
- Dangerous characters filtered (<, >, ", ', \, ;, $, %, &)
- Max 100 keywords per order
- Media type must exist and be configured

## Error Handling

**Strict policy:**
- For validation errors â†’ Show specific error, guide user to fix
- For project status errors â†’ Notify çˆ¬è™«å·¥ç¨‹å¸ˆ via Feishu
- For API failures â†’ Notify çˆ¬è™«å·¥ç¨‹å¸ˆ via Feishu
- NEVER attempt to fix technical issues automatically

**Error notification:**
```python
from scripts.feishu_client import FeishuClient

feishu = FeishuClient()
feishu.send_notification(
    title="ğŸ†˜ğŸ†˜ğŸ†˜çˆ¬è™«ä»»åŠ¡å¤±è´¥ğŸ†˜ğŸ†˜ğŸ†˜",
    text=f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆ\né”™è¯¯: {error_message}",
    at_user=["ou_a45583a7f2843869b71ff4cc9692cf3d"]
)
```

## Message Field Types (CRITICAL)

Different PySpider crawlers expect different message fields:

- **keyword** - Single keyword (Reddit, TikTok, Youtube, Pinterest)
- **keywords** - Multiple keywords (Amazon, Facebook Group)
- **tags** - Hashtag/tag (Instagram)
- **brand** - Brand name (SellerSprite)
- **seller** - Seller ID (SellerSprite)
- **url** - URL-based (Facebook Ads, TikTok User)
- **nodeIdPath** - Category path (SellerSprite Category)

The `create_crawl_order.py` script automatically handles this based on CSV config.

## Interaction Examples

**Example 1: User request unclear**
```
User: "å¸®æˆ‘æŠ“æ•°æ®"
Agent: "æˆ‘å¯ä»¥å¸®ä½ æŠ“å–ä»¥ä¸‹å¹³å°çš„æ•°æ®ï¼š
       
       ğŸ“± ç¤¾äº¤åª’ä½“ï¼š
       â€¢ Reddit - å…³é”®è¯ä¸‹çš„å¸–å­
       â€¢ Instagram - æ ‡ç­¾ä¸‹çš„å¸–å­
       â€¢ TikTok - æ ‡ç­¾ä¸‹çš„å¸–å­
       â€¢ Twitter - å…³é”®è¯ä¸‹çš„å¸–å­
       â€¢ Facebook - å¹¿å‘Š/ç¾¤ç»„
       
       ğŸ›’ ç”µå•†ï¼š
       â€¢ Amazon - äº§å“è¯„è®º
       â€¢ å–å®¶ç²¾çµ - å“ç‰Œ/å–å®¶/å…³é”®è¯é”€å”®é¢
       
       ğŸ” SEOå·¥å…·ï¼š
       â€¢ SEMrush - å¤–é“¾æ•°æ®
       
       å‘Šè¯‰æˆ‘ä½ æƒ³æŠ“å“ªä¸ªå¹³å°ï¼Œå…³é”®è¯æ˜¯ä»€ä¹ˆï¼Ÿ"

User: "æŠ“Redditï¼Œå…³é”®è¯æ˜¯AI"
Agent: [Parse â†’ Validate â†’ Confirm â†’ Execute]
       "ç¡®è®¤æŠ“å–ä»»åŠ¡ï¼š
        â€¢ åª’ä½“: Reddit å…³é”®è¯ä¸‹çš„å¸–å­
        â€¢ å…³é”®è¯: AI
        â€¢ é¡¹ç›®: ScrapingRedditByKeyword_api
        ç¡®è®¤æ‰§è¡Œå—ï¼Ÿ"
```

**Example 2: Clear request with keyword**
```
User: "æŠ“Redditä¸Šå…³äºAIçš„å¸–å­"
Agent: [Parse â†’ Validate â†’ Confirm â†’ Execute]
       "ç¡®è®¤æŠ“å–ä»»åŠ¡ï¼š
        â€¢ åª’ä½“: Reddit å…³é”®è¯ä¸‹çš„å¸–å­
        â€¢ å…³é”®è¯: AI
        â€¢ é¡¹ç›®: ScrapingRedditByKeyword_api
        ç¡®è®¤æ‰§è¡Œå—ï¼Ÿ"
```

**Example 3: Request without keyword**
```
User: "æŠ“SEMrushå¤–é“¾æ•°æ®"
Agent: "å¥½çš„ï¼ŒSEMrushå¤–é“¾æ•°æ®ã€‚è¯·è¾“å…¥è¦æŸ¥è¯¢çš„åŸŸåï¼š"

User: "example.com"
Agent: [Validate â†’ Confirm â†’ Execute]
       "ç¡®è®¤æŠ“å–ä»»åŠ¡ï¼š
        â€¢ åª’ä½“: SEMrushå¤–é“¾æ•°æ®
        â€¢ åŸŸå: example.com
        ç¡®è®¤æ‰§è¡Œå—ï¼Ÿ"
```

**Example 4: User wants custom platform not in list**
```
User: "æˆ‘è¦æŠ“Pinterestæ•°æ®"
Agent: "å¥½çš„ï¼ŒPinterestæ•°æ®ã€‚å…³é”®è¯æ˜¯ä»€ä¹ˆï¼Ÿ"

User: "travel"
Agent: [Validate â†’ Confirm â†’ Execute]
       "ç¡®è®¤æŠ“å–ä»»åŠ¡ï¼š
        â€¢ åª’ä½“: Pinterest
        â€¢ å…³é”®è¯: travel
        ç¡®è®¤æ‰§è¡Œå—ï¼Ÿ"
```

## IMPORTANT: Interaction Pattern

**âœ… DO - é€æ­¥è¯¢é—®ï¼Œè‡ªç„¶å¯¹è¯**ï¼š
1. å±•ç¤ºé€‰é¡¹å¸®åŠ©ç”¨æˆ·ç†è§£ï¼ˆä¸è¦ä¸€æ¬¡æ€§å±•ç¤ºå¤ªå¤šï¼Œåˆ†ç±»å±•ç¤ºï¼‰
2. æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜
3. ç­‰å¾…ç”¨æˆ·å›å¤åï¼Œå†é—®ä¸‹ä¸€ä¸ªé—®é¢˜
4. å…è®¸ç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¸è¦å¼ºåˆ¶é€‰æ‹©é€‰é¡¹
5. æ™ºèƒ½è§£æå¤šå…³é”®è¯ï¼šæ”¯æŒç©ºæ ¼ã€é€—å·ã€é¡¿å·ã€æ¢è¡Œç­‰åˆ†éš”ç¬¦
6. æœ€åä¸€æ¬¡æ€§ç¡®è®¤æ‰€æœ‰ä¿¡æ¯

**âŒ DON'T - é¿å…è¿™äº›é”™è¯¯åšæ³•**ï¼š
1. ä¸è¦ç”¨"å…¶ä»–"é€‰é¡¹ - ä¼šå¯¼è‡´æ­»é”ï¼Œç”¨æˆ·æ— æ³•è¾“å…¥
2. ä¸è¦ä¸€æ¬¡æ€§é—®æ‰€æœ‰é—®é¢˜ - ç»™ç”¨æˆ·å‹åŠ›
3. ä¸è¦ç”¨ AskUserQuestion æä¾›é€‰é¡¹åï¼Œæ— æ³•ç»§ç»­è¾“å…¥
4. ä¸è¦å¼ºåˆ¶ç”¨æˆ·æŒ‰ç…§æ ¼å¼è¾“å…¥ - å…ˆå°è¯•ç†è§£è‡ªç„¶è¯­è¨€
5. ä¸è¦æŠŠé€—å·åˆ†éš”çš„å…³é”®è¯å½“ä½œå•ä¸ªå…³é”®è¯

**Pattern**:
- å¦‚æœç”¨æˆ·åœ¨åˆå§‹è¯·æ±‚ä¸­æä¾›äº†å…³é”®è¯ â†’ æ™ºèƒ½è§£æï¼ˆæ”¯æŒå¤šå…³é”®è¯ï¼‰â†’ ç›´æ¥æ ¡éªŒå¹¶ç¡®è®¤
- å¦‚æœç”¨æˆ·æ²¡æä¾›å…³é”®è¯ â†’ å…ˆå±•ç¤ºé€‰é¡¹ï¼Œç„¶åè¯¢é—®"å‘Šè¯‰æˆ‘ä½ æƒ³æŠ“å“ªä¸ªå¹³å°ï¼Œå…³é”®è¯æ˜¯ä»€ä¹ˆï¼Ÿ"
- æ”¯æŒç”¨æˆ·è¯´"AIã€machine learningã€crypto"æˆ–"AI, machine learning, crypto"
- è‡ªåŠ¨è§£ææˆåˆ—è¡¨ï¼š`["AI", "machine learning", "crypto"]`
- ä¿æŒå¯¹è¯è¿ç»­æ€§ï¼ŒåƒçœŸäººå¯¹è¯ä¸€æ ·

**After Order Success Response:**
```
âœ… ä¸‹å•æˆåŠŸï¼

ä»»åŠ¡ID: xxx
é£ä¹¦è®°å½•ID: xxx
é¡¹ç›®: xxx

ğŸ“¤ å·²é€šçŸ¥é£ä¹¦ç¾¤ï¼š@ä½  @çˆ¬è™«å·¥ç¨‹å¸ˆ

æ­£åœ¨æŠ“å–ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼Œå®Œæˆåä¼šé€šè¿‡é£ä¹¦é€šçŸ¥ä½ ç»“æœğŸ“¬
```

## Workflow Summary

1. **Understand** - Parse user request (extract media type + keywords if provided)
2. **Guide** - If unclear, show categorized crawler options (ç¤¾äº¤åª’ä½“/ç”µå•†/SEOå·¥å…·)
3. **Ask** - One question at a time, wait for user response
4. **Validate** - Use `run.py validate` strictly
5. **Confirm** - Show all collected info and ask for final confirmation
6. **Execute** - Use `run.py order` (handles all steps)
7. **Notify** - Success/failure notifications via Feishu

**NEVER skip steps or write custom scripts. Always use run.py for all operations.**
