"""åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„çˆ¬è™«ç±»å‹ä¾›ç”¨æˆ·é€‰æ‹©"""
import csv
import os

def list_all_crawlers(csv_path=None):
    """
    åˆ—å‡ºæ‰€æœ‰é…ç½®çš„çˆ¬è™«ç±»å‹ï¼ŒæŒ‰åˆ†ç±»å±•ç¤º
    
    Returns:
        dict: {
            'ç¤¾äº¤åª’ä½“': [{name, project, field, complete, example}],
            'ç”µå•†å¹³å°': [...],
            'SEOå·¥å…·': [...]
        }
    """
    if csv_path is None:
        # CSVåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼ˆç”¨æˆ·å®‰è£…skillåçš„å·¥ä½œç›®å½•ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # skillç›®å½•: project_root/.minto/skills/pyspider-order/skills/
        # é¡¹ç›®æ ¹ç›®å½•: å‘ä¸Š4çº§
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(script_dir))))
        csv_path = os.path.join(project_root, 'feishudb.ScrapingMongoQuery.csv')
    
    categories = {
        "ç¤¾äº¤åª’ä½“": [],
        "ç”µå•†å¹³å°": [],
        "SEOå·¥å…·": [],
        "å’¨è¯¢ä»»åŠ¡": []
    }
    
    # å…³é”®è¯ç¤ºä¾‹æ˜ å°„
    examples = {
        "Reddit": ("AI", "machine learning"),
        "Instagram": ("fashion", "travel"),
        "TikTok": ("crypto", "dance"),
        "Twitter": ("tech", "news"),
        "Facebook": ("https://www.facebook.com/example",),
        "Youtube": ("music", "gaming"),
        "Pinterest": ("food", "DIY"),
        "Amazon": ("iPhone", "laptop"),
        "å–å®¶ç²¾çµ": ("Nike", "Apple"),
        "SEMrush": ("example.com",)
    }
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        
        for row in reader:
            name = row.get('name', '')
            scrap_key = row.get('scrap_key', '')
            table = row.get('table', '')
            has_mongo = bool(row.get('mongo_list[0]', ''))
            
            # è·å–ç¤ºä¾‹
            example_keywords = []
            for key, vals in examples.items():
                if key in name:
                    example_keywords = vals
                    break
            
            crawler_info = {
                "name": name,
                "project": table,
                "field": scrap_key,
                "complete": has_mongo,
                "examples": example_keywords
            }
            
            # åˆ†ç±»
            if any(k in name for k in ['Reddit', 'Instagram', 'TikTok', 'Twitter', 'Facebook', 'Youtube', 'Pinterest']):
                categories["ç¤¾äº¤åª’ä½“"].append(crawler_info)
            elif any(k in name for k in ['Amazon', 'å–å®¶ç²¾çµ']):
                categories["ç”µå•†å¹³å°"].append(crawler_info)
            elif any(k in name for k in ['semrush', 'SEMrush', 'Google']):
                categories["SEOå·¥å…·"].append(crawler_info)
            elif 'å…¨æ¡ˆå’¨è¯¢' in name:
                categories["å’¨è¯¢ä»»åŠ¡"].append(crawler_info)
    
    return categories


def format_crawlers_for_display(categories):
    """
    æ ¼å¼åŒ–çˆ¬è™«åˆ—è¡¨ç”¨äºæ˜¾ç¤ºç»™ç”¨æˆ·
    
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    lines = []
    lines.append("=" * 100)
    lines.append("ğŸš€ å¯ç”¨çš„çˆ¬è™«ä»»åŠ¡ç±»å‹")
    lines.append("=" * 100)
    
    for category, crawlers in categories.items():
        if crawlers:
            lines.append(f"\nğŸ“Œ {category}")
            lines.append("-" * 100)
            for i, c in enumerate(crawlers, 1):
                status = "âœ…" if c['complete'] else "âš ï¸ "
                lines.append(f"{status} {i}. {c['name']}")
                lines.append(f"   â””â”€ é¡¹ç›®: {c['project']}")
                lines.append(f"   â””â”€ å­—æ®µç±»å‹: {c['field']}")
                
                # æ˜¾ç¤ºç¤ºä¾‹
                if c['examples']:
                    examples_str = "ã€".join(c['examples'][:3])
                    lines.append(f"   â””â”€ ç¤ºä¾‹: {examples_str}")
                
                if not c['complete']:
                    lines.append(f"   â””â”€ âš ï¸  é…ç½®ä¸å®Œæ•´ï¼Œæš‚æœªè‡ªåŠ¨åŒ–")
    
    lines.append("\n" + "=" * 100)
    lines.append("ğŸ’¡ æç¤ºï¼šè¯·å‘Šè¯‰æˆ‘ä½ æƒ³æŠ“å“ªä¸ªå¹³å°çš„æ•°æ®ï¼Œä»¥åŠå…³é”®è¯")
    lines.append("   ä¾‹å¦‚ï¼š\"æŠ“Redditä¸Šå…³äºAIçš„å¸–å­\" æˆ– \"æŸ¥Amazonä¸ŠiPhoneçš„è¯„è®º\"")
    lines.append("=" * 100)
    
    return "\n".join(lines)


if __name__ == "__main__":
    # æµ‹è¯•
    categories = list_all_crawlers()
    print(format_crawlers_for_display(categories))
