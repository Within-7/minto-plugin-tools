#!/usr/bin/env python3
"""PySpiderçˆ¬è™«ä¸‹å• - æç®€ç‰ˆæœ¬

åŠŸèƒ½ï¼š
1. æ ¡éªŒå‚æ•°ï¼ˆFacebook URLæ ¼å¼ï¼‰
2. æ£€æŸ¥PySpideré¡¹ç›®çŠ¶æ€
3. åˆ›å»ºé£ä¹¦è®°å½•
4. å‘é€PySpiderä»»åŠ¡
5. æ›´æ–°é£ä¹¦çŠ¶æ€å¹¶å‘é€é€šçŸ¥
"""
import sys
import os
import uuid
from pathlib import Path

# æ·»åŠ æ’ä»¶æ ¹ç›®å½•åˆ°è·¯å¾„
PLUGIN_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PLUGIN_ROOT))

from scripts.feishu_client import FeishuClient
from scripts.pyspider_dispatcher import PySpiderDispatcher
from scripts.check_project_status import check_project_status
from scripts.crawlers import get_crawler_info, list_crawlers, format_crawlers_for_display


def validate_facebook_url(url: str) -> bool:
    """Facebook URLå¿…é¡»ä»¥ https://www.facebook.com/ å¼€å¤´"""
    return url.startswith('https://www.facebook.com/')


def create_order(media_type: str, keywords: list, task_user: str = None) -> dict:
    """
    åˆ›å»ºçˆ¬è™«è®¢å•
    
    Args:
        media_type: åª’ä½“ç±»å‹ï¼ˆå¦‚ "Reddit å…³é”®è¯ä¸‹çš„å¸–å­"ï¼‰
        keywords: å…³é”®è¯åˆ—è¡¨
        task_user: é£ä¹¦ç”¨æˆ·ID
        
    Returns:
        dict: {'success': bool, 'message': str}
    """
    try:
        # 0. ç¯å¢ƒå˜é‡æ£€æŸ¥ï¼ˆå¼€å‘ç¯å¢ƒæœ‰é»˜è®¤å€¼ï¼Œå¯è·³è¿‡ï¼‰
        # ç”Ÿäº§ç¯å¢ƒå»ºè®®é…ç½®ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤å€¼
        # ç”Ÿäº§ç¯å¢ƒå¿…é¡»é…ç½®çš„ç¯å¢ƒå˜é‡ï¼šMONGODB_URL, FEISHU_API_URL, PYSPIDER_BASE_URL
        debug_mode = os.getenv('MINTO_DEBUG', 'false').lower() == 'true'
        
        if debug_mode:
            print("[DEBUG] Running in debug mode (environment variable check skipped)")
        
        # 1. è·å–çˆ¬è™«é…ç½®
        crawler_info = get_crawler_info(media_type)
        if not crawler_info:
            # çˆ¬è™«ä¸å­˜åœ¨ï¼Œæ˜¾ç¤ºå¯ç”¨åˆ—è¡¨
            available = format_crawlers_for_display()
            return {
                'success': False,
                'message': f'âŒ ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {media_type}\\n\\n{available}'
            }
        
        project_name = crawler_info["project"]
        field_type = crawler_info["field"]
        
        # 2. å‚æ•°æ ¡éªŒï¼ˆä»…Facebook Adséœ€è¦ç‰¹æ®Šæ ¡éªŒï¼‰
        if "validation" in crawler_info and crawler_info["validation"]:
            for kw in keywords:
                if not validate_facebook_url(kw):
                    return {
                        'success': False,
                        'message': f'âŒ Facebook URLæ ¼å¼é”™è¯¯: {kw}\\nå¿…é¡»ä»¥ https://www.facebook.com/ å¼€å¤´'
                    }
        
        # 3. æ£€æŸ¥é¡¹ç›®çŠ¶æ€
        status_info = check_project_status(project_name)
        
        if not status_info['exists']:
            return {
                'success': False,
                'message': f'âŒ PySpideré¡¹ç›®ä¸å­˜åœ¨: {project_name}'
            }
        
        if not status_info['can_run']:
            return {
                'success': False,
                'message': f'âŒ PySpideré¡¹ç›®çŠ¶æ€å¼‚å¸¸: {status_info["status"]}\\né¡¹ç›®å¿…é¡»å¤„äº RUNNING æˆ– DEBUG çŠ¶æ€'
            }
        
        # 4. åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆç¯å¢ƒå˜é‡å·²åœ¨å®¢æˆ·ç«¯æ„é€ å‡½æ•°ä¸­éªŒè¯ï¼‰
        feishu = FeishuClient()
        dispatcher = PySpiderDispatcher()
        task_id = str(uuid.uuid4())
        
        # 5. åˆ›å»ºé£ä¹¦è®°å½•
        print(f"[DEBUG] Creating Feishu record for {media_type}...")
        record_id = feishu.create_record(
            task=media_type,
            data=keywords,
            task_user=task_user or 'system',
            task_id=task_id
        )
        
        if not record_id:
            print(f"[DEBUG] Feishu record creation failed")
            return {
                'success': False,
                'message': 'âŒ é£ä¹¦è®°å½•åˆ›å»ºå¤±è´¥ï¼Œè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆ\nå¯èƒ½åŸå› ï¼šé£ä¹¦APIå¼‚å¸¸ã€è¡¨æ ¼æƒé™é—®é¢˜ã€å­—æ®µé…ç½®é”™è¯¯'
            }
        
        # 6. å‘é€PySpiderä»»åŠ¡
        print(f"[DEBUG] Sending PySpider tasks for {len(keywords)} keywords...")
        success_count = 0
        
        for keyword in keywords:
            print(f"[DEBUG] Sending task for keyword: {keyword}...")
            try:
                if dispatcher.send_task(project_name, field_type, keyword):
                    success_count += 1
                    print(f"[DEBUG] âœ“ Task sent successfully for: {keyword}")
                else:
                    print(f"[DEBUG] âœ— Task send failed for: {keyword}")
            except Exception as e:
                print(f"[DEBUG] Exception during task send: {e}")
        
        if success_count == 0:
            print(f"[DEBUG] All {len(keywords)} tasks failed, marking as manual processing")
            feishu.update_status(record_id, "ç­‰å¾…æ‰‹åŠ¨å¤„ç†")
            return {
                'success': False,
                'message': f'âŒ PySpiderä»»åŠ¡å‘é€å¤±è´¥ï¼ˆ{success_count}/{len(keywords)} æˆåŠŸï¼‰\nè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆæ£€æŸ¥PySpideræœåŠ¡å’Œç½‘ç»œè¿æ¥'
            }
        
        # 7. æ›´æ–°é£ä¹¦çŠ¶æ€ä¸º"æŠ“å–ä¸­"
        print(f"[DEBUG] Updating status to 'æŠ“å–ä¸­'...")
        feishu.update_status(record_id, "æŠ“å–ä¸­")
        print(f"[DEBUG] Status updated successfully")
        
        # 8. å‘é€é£ä¹¦é€šçŸ¥
        print(f"[DEBUG] Sending Feishu notification...")
        feishu.send_notification(
            title="[Minto] ğŸ’£ğŸ’£ğŸ’£å¼€å§‹æŠ“å–ğŸ’£ğŸ’£ğŸ’£",
            text=f"åª’ä½“:ã€{media_type}ã€‘\\nå…³é”®è¯: {keywords}\\n\\né€šè¿‡ Minto è‡ªåŠ¨åŒ–æ’ä»¶ä¸‹å•",
            at_user=[task_user] if task_user else ['all']
        )
        
        return {
            'success': True,
            'message': f'âœ… ä¸‹å•æˆåŠŸï¼\\nä»»åŠ¡ID: {task_id}\\né£ä¹¦è®°å½•ID: {record_id}\\né¡¹ç›®: {project_name}\\næˆåŠŸå‘é€ {success_count}/{len(keywords)} ä¸ªå…³é”®è¯'
        }
        
    except Exception as e:
        return {
            'success': False,
            'message': f'âŒ ä¸‹å•å¤±è´¥: {str(e)}'
        }


if __name__ == "__main__":
    # æµ‹è¯•
    result = create_order(
        media_type="Reddit å…³é”®è¯ä¸‹çš„å¸–å­",
        keywords=["AI", "machine learning"],
        task_user="ou_test"
    )
    print(result['message'])
