---
name: pyspider-order
description: "Manage PySpider web scraping tasks through natural language. Use when analysts request: (1) Scraping social media data (Reddit, Instagram, TikTok, Twitter, Facebook), (2) Keyword-based content collection, (3) E-commerce data scraping (Amazon, SellerSprite), (4) SEO data extraction (SEMrush). Maps natural language requests to PySpider projects via Feishu API integration."
---

# PySpider Order

Enable analysts to order web scraping tasks through natural language conversation.

## âš ï¸ CRITICAL: Interaction Rules

**MUST follow these constraints:**

1. **NEVER write test scripts automatically** - Always use pre-built scripts from scripts/
2. **NEVER execute without confirmation** - Must show confirmation dialog before any API calls
3. **ALWAYS parse natural language first** - Extract media type and keywords from user request
4. **CHECK message field type** - Different crawlers use different message fields (keyword/keywords/tags/url/brand/seller)
5. **VALIDATE parameters strictly** - Use scripts/validate_params.py for all user inputs
6. **NEVER access MongoDB directly** - Only through scripts/check_project_status.py (internal use)

## ğŸš¨ CRITICAL: Debugging Rules (NEW!)

**å½“ç”¨æˆ·æŠ¥å‘Šé—®é¢˜æˆ–è¦æ±‚ä¿®å¤bugæ—¶ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹æµç¨‹ï¼š**

### ç¦æ­¢è¡Œä¸ºï¼ˆDO NOT DOï¼‰ï¼š
- âŒ **ä¸è¦ç›²ç›®æ‰§è¡Œæµ‹è¯•ä»£ç ** - é—®é¢˜åˆ†æå’Œä»£ç å®¡æŸ¥åº”è¯¥åœ¨å¤§è„‘ä¸­å®Œæˆ
- âŒ **ä¸è¦åå¤bashè¿è¡Œ** - æ¯æ¬¡è¿è¡Œå¯èƒ½è§¦å‘å®é™…ä¸šåŠ¡æ“ä½œï¼ˆå¦‚å‘é€çˆ¬è™«ä»»åŠ¡ï¼‰
- âŒ **ä¸è¦è¾¹è¯•è¾¹æ”¹** - è¿™æ˜¯ä½æ•ˆçš„ï¼Œå®¹æ˜“äº§ç”Ÿå‰¯ä½œç”¨
- âŒ **ä¸è¦å¿½è§†ç”¨æˆ·æŒ‡ä»¤** - å¦‚æœç”¨æˆ·è¯´"åˆ«æ‰§è¡Œ"ã€"å…ˆåˆ†æ"ï¼Œå¿…é¡»ç«‹å³åœæ­¢æ‰§è¡Œ

### å¿…é¡»éµå®ˆçš„æµç¨‹ï¼ˆMUST FOLLOWï¼‰ï¼š

**é˜¶æ®µ1ï¼šç†è§£é—®é¢˜ï¼ˆåªè¯»ï¼Œä¸æ‰§è¡Œï¼‰**
1. ä»”ç»†é˜…è¯»ç›¸å…³ä»£ç æ–‡ä»¶
2. ç†è§£ç°æœ‰çš„ä¸šåŠ¡æµç¨‹å’Œæ•°æ®æµ
3. æ‰¾å‡ºé—®é¢˜å¯èƒ½çš„åŸå› 
4. åˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ

**é˜¶æ®µ2ï¼šæ–¹æ¡ˆç¡®è®¤ï¼ˆä¸ç”¨æˆ·æ²Ÿé€šï¼‰**
1. å‘ç”¨æˆ·è§£é‡Šä½ çš„åˆ†æç»“æœ
2. æå‡ºå¤šä¸ªè§£å†³æ–¹æ¡ˆï¼ˆå¦‚æœæœ‰ï¼‰
3. è¯´æ˜æ¯ä¸ªæ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹
4. **ç­‰å¾…ç”¨æˆ·ç¡®è®¤åå†æ‰§è¡Œ**

**é˜¶æ®µ3ï¼šå®æ–½ä¿®æ”¹ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰**
1. åŸºäºç¡®è®¤çš„æ–¹æ¡ˆï¼Œç›´æ¥ä¿®æ”¹ä»£ç 
2. ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰ç›¸å…³ä¿®æ”¹
3. å‘ç”¨æˆ·è¯´æ˜æ”¹äº†ä»€ä¹ˆã€ä¸ºä»€ä¹ˆè¿™æ ·æ”¹

### å…·ä½“æ¡ˆä¾‹ - é£ä¹¦è®°å½•åˆ›å»ºé—®é¢˜ï¼š

**é”™è¯¯åšæ³•ï¼ˆå·²çŠ¯ï¼‰ï¼š**
```
ç”¨æˆ·ï¼šé£ä¹¦è®°å½•æ²¡åˆ›å»º
æˆ‘ï¼šbashæµ‹è¯• â†’ å‘ç°å­—æ®µä¸å­˜åœ¨ â†’ æ”¹å­—æ®µ â†’ bashæµ‹è¯• â†’ åˆå¤±è´¥ â†’ æ”¹å­—æ®µ â†’ ...
åæœï¼šå‘é€äº†å¤šæ¬¡é‡å¤çš„çˆ¬è™«ä»»åŠ¡
```

**æ­£ç¡®åšæ³•ï¼ˆåº”è¯¥ï¼‰ï¼š**
```
ç”¨æˆ·ï¼šé£ä¹¦è®°å½•æ²¡åˆ›å»º
æˆ‘ï¼š
1. é˜…è¯» feishu.py ç¬¬962-1010è¡Œï¼Œç†è§£åˆ›å»ºæµç¨‹
2. é˜…è¯» pyspider_crawl.pyï¼Œç†è§£ä¸šåŠ¡æµç¨‹
3. åˆ†æï¼šç°æœ‰æµç¨‹æ˜¯"å…ˆåˆ›å»ºè®°å½•ï¼Œåå‘çˆ¬è™«"
4. å‘ç°é—®é¢˜ï¼šé£ä¹¦å­—æ®µIDå’Œæ˜¾ç¤ºåä¸ä¸€è‡´
5. å‘ç”¨æˆ·è¯´æ˜é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
6. ç­‰å¾…ç”¨æˆ·ç¡®è®¤åï¼Œä¸€æ¬¡æ€§ä¿®æ”¹ä»£ç 
```

### è‡ªæ£€æ¸…å•ï¼š
åœ¨æ‰§è¡Œä»»ä½•bashå‘½ä»¤å‰ï¼Œé—®è‡ªå·±ï¼š
- [ ] è¿™ä¸ªå‘½ä»¤ä¼šè§¦å‘å®é™…ä¸šåŠ¡æ“ä½œå—ï¼Ÿï¼ˆå‘æ¶ˆæ¯ã€å†™æ•°æ®åº“ã€è°ƒç”¨APIï¼‰
- [ ] ç”¨æˆ·å·²ç»æ˜ç¡®è¦æ±‚æˆ‘æ‰§è¡Œäº†å—ï¼Ÿ
- [ ] æˆ‘æœ‰æ²¡æœ‰å……åˆ†åˆ†æé—®é¢˜çš„æ ¹æºï¼Ÿ
- [ ] æœ‰æ²¡æœ‰æ›´å®‰å…¨çš„åˆ†ææ–¹æ³•ï¼ˆåªè¯»æ–‡ä»¶ã€æŸ¥çœ‹æ—¥å¿—ï¼‰ï¼Ÿ

**å¦‚æœç­”æ¡ˆéƒ½æ˜¯"æ˜¯"ï¼Œæ‰èƒ½æ‰§è¡Œã€‚å¦åˆ™ï¼Œå…ˆåˆ†æï¼Œå†æ²Ÿé€šï¼Œæœ€åæ‰§è¡Œã€‚**

## Core Capabilities

### 1. List Available Crawlers

When user request is unclear or wants to see options:

**Use:** `scripts/list_all_crawlers.py`

```python
from scripts.list_all_crawlers import list_all_crawlers, format_crawlers_for_display

categories = list_all_crawlers()
print(format_crawlers_for_display(categories))
```

This displays all 20+ crawlers organized by category with examples.

### 2. Order Crawl Task

Parse natural language requests and create scraping tasks:

**MANDATORY Workflow:**
1. Parse user's natural language request â†’ Extract media type + keywords
2. If unclear, use `list_all_crawlers.py` to show options
3. Ask for user's Feishu open_id (optional but recommended for proper attribution)
4. Validate parameters using `scripts/validate_params.py`
5. Show confirmation dialog using AskUserQuestion
6. Only after user confirms, use `scripts/create_crawl_order.py` to execute

**Execute order:**
```python
from scripts.create_crawl_order import create_crawl_order, format_order_result

result = create_crawl_order(
    media_type="Reddit å…³é”®è¯ä¸‹çš„å¸–å­",
    keywords="AI",
    task_user="ou_xxxxxxxxxxxxx"  # Optional: User's Feishu open_id
)
print(format_order_result(result))
```

**âš ï¸ IMPORTANT: Feishu Field Mapping**
The plugin uses Chinese field names (not English) for Feishu Bitable:
- `ä»»åŠ¡ç±»å‹` â† media_type (e.g., "Reddit å…³é”®è¯ä¸‹çš„å¸–å­")
- `å…³é”®è¯1`, `å…³é”®è¯2`, ... â† keywords array
- `æ•°æ®æŠ“å–çŠ¶æ€` â† Fixed value: "ç­‰å¾…å¤„ç†"
- `ç´§æ€¥ç¨‹åº¦` â† Fixed value: "ä¸€èˆ¬ï¼ˆä»Šå¤©ï¼‰"
- `å·¥å•å‘èµ·äºº` â† Optional: user object {id, name}
- `chargeä»»åŠ¡` â† Optional: user ID array

**User Attribution:**
- If `task_user` is provided: sets å·¥å•å‘èµ·äºº and chargeä»»åŠ¡
- If not provided: Feishu system uses default values (may show as "ä»»åŠ¡åˆ†å‘")
- Recommended: Always ask user for their Feishu open_id for proper tracking

### 3. Check Task Progress

Query Feishu API for task status:

**Use:** `scripts/query_task_progress.py`

```python
from scripts.query_task_progress import query_all_tasks, format_tasks_for_display

tasks = query_all_tasks()
print(format_tasks_for_display(tasks, show_all=False))
```

## Pre-built Scripts (NEVER rewrite these)

**Query & Display:**
- `scripts/list_all_crawlers.py` - List all available crawlers with examples
- `scripts/query_task_progress.py` - Query Feishu task status

**Validation:**
- `scripts/validate_params.py` - Strict parameter validation (URL, keywords, etc.)

**Execution:**
- `scripts/create_crawl_order.py` - Complete order workflow (validation â†’ status check â†’ create â†’ dispatch)
- `scripts/check_project_status.py` - Check PySpider project status (internal use)
- `scripts/feishu_client.py` - Feishu API client
- `scripts/pyspider_dispatcher.py` - PySpider dispatcher client

## Parameter Validation

**Always use `scripts/validate_params.py`:**

```python
from scripts.validate_params import validate_crawl_params, ValidationError

try:
    validated = validate_crawl_params(media_type, keywords)
    # validated contains:
    # - media_type, project, field, keywords, validated
except ValidationError as e:
    # Show error to user, guide them to fix
    print(str(e))
```

**Validation rules:**
- URL format (Facebook Ads must start with https://www.facebook.com/)
- Keyword length (max 500 chars)
- Dangerous characters filtered (<, >, ", ', \, ;, $, %, &)
- Max 100 keywords per order
- Media type must exist and be configured

## Error Handling

**Strict security policy:**
- For validation errors â†’ Show specific error, guide user to fix
- For project status errors â†’ Notify çˆ¬è™«å·¥ç¨‹å¸ˆ via Feishu
- For API failures â†’ Notify çˆ¬è™«å·¥ç¨‹å¸ˆ via Feishu
- NEVER attempt to fix technical issues automatically

**Error notification:**
```python
from scripts.feishu_client import FeishuClient

feishu = FeishuClient()
feishu.send_notification(
    title="ğŸ†˜ğŸ†˜ğŸ†˜çˆ¬è™«ä»»åŠ¡å¤±è´¥ğŸ†˜ğŸ†˜ğŸ†˜",
    text=f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·è”ç³»çˆ¬è™«å·¥ç¨‹å¸ˆ\né”™è¯¯: {error_message}",
    at_user=["ou_a45583a7f2843869b71ff4cc9692cf3d"]
)
```

## Message Field Types (CRITICAL)

Different PySpider crawlers expect different message fields in `on_message()`:

- **keyword** - Single keyword (Reddit, TikTok, Youtube, Pinterest)
- **keywords** - Multiple keywords (Amazon, Facebook Group)
- **tags** - Hashtag/tag (Instagram)
- **brand** - Brand name (SellerSprite)
- **seller** - Seller ID (SellerSprite)
- **url** - URL-based (Facebook Ads, TikTok User)
- **nodeIdPath** - Category path (SellerSprite Category)

The `create_crawl_order.py` script automatically handles this based on CSV config.

## Media Type Mapping

See [references/media_mapping.md](references/media_mapping.md) for complete mapping.

**Quick reference:**

| Natural Language | Feishu Media Type | PySpider Project | Message Field |
|-----------------|-------------------|------------------|---------------|
| Reddit | Reddit å…³é”®è¯ä¸‹çš„å¸–å­ | ScrapingRedditByKeyword_api | keyword |
| Instagram | Instagram æ ‡ç­¾ä¸‹çš„å¸–å­ | ScrapingInstagramPostsFromTagsSearch | tags |
| TikTok | TikTok æ ‡ç­¾ä¸‹çš„å¸–å­ | ScrapingTiktokByKeywordsFromBD | keyword |
| Twitter | Twitter å…³é”®è¯ä¸‹çš„å¸–å­ | ScrapingTwitterPostsByTags | keyword |
| Amazon | Amazonåˆ—è¡¨æ‰€æœ‰äº§å“åŠè¯„è®º | ScrapingAmazonListByKeywords | keywords |
| å–å®¶ç²¾çµå“ç‰Œ | å–å®¶ç²¾çµçš„å“ç‰Œé”€å”®é¢æ•°æ® | ScrapingMjjlDispatcherByBrand | brand |

## Supported Crawlers (20+)

**Social Media:** Reddit, Instagram, TikTok, Twitter, Facebook Ads/Group, Youtube, Pinterest

**E-commerce:** Amazon, SellerSprite (brand/seller/keyword/category)

**SEO Tools:** SEMrush BackLink

**Other:** Google Index tasks, Pinterest profiles

See [references/media_mapping.md](references/media_mapping.md) for complete list.

## Interaction Examples

**Example 1: User request unclear**
```
User: "å¸®æˆ‘æŠ“æ•°æ®"
Agent: [Use list_all_crawlers.py] 
       "æˆ‘å¯ä»¥å¸®ä½ æŠ“å–ä»¥ä¸‹å¹³å°çš„æ•°æ®...
       è¯·å‘Šè¯‰æˆ‘ä½ æƒ³æŠ“å“ªä¸ªå¹³å°ï¼Ÿ"
```

**Example 2: Clear request with keyword**
```
User: "æŠ“Redditä¸Šå…³äºAIçš„å¸–å­"
Agent: [Parse â†’ Validate â†’ Confirm â†’ Execute]
       "ç¡®è®¤æŠ“å–ä»»åŠ¡
        åª’ä½“: Reddit å…³é”®è¯ä¸‹çš„å¸–å­
        å…³é”®è¯: AI
        é¡¹ç›®: ScrapingRedditByKeyword_api
        [ç¡®è®¤æ‰§è¡Œ] [å–æ¶ˆ]"
```

**Example 3: Request without keyword**
```
User: "æŠ“SEMrushå¤–é“¾æ•°æ®"
Agent: "è¯·è¾“å…¥è¦æŸ¥è¯¢çš„å…³é”®è¯ï¼ˆåŸŸåï¼‰ï¼š"
User: "example.com"
Agent: [Confirm and execute]
```

**Example 4: Check progress**
```
User: "æŸ¥ä¸€ä¸‹ä»»åŠ¡è¿›åº¦"
Agent: [Use query_task_progress.py]
       Shows tasks grouped by status
```

**IMPORTANT: Keyword Input Pattern**
- If user provides keyword in initial request â†’ Validate directly
- If user selects "å…¶ä»–" (other) or doesn't provide keyword â†’ Ask for keyword input
- Never use AskUserQuestion with "å…¶ä»–å…³é”®è¯" option that doesn't allow input
- Instead: Ask directly "è¯·è¾“å…¥å…³é”®è¯" or use multi-step conversation

## Workflow Summary

1. **Understand** - Parse user request (extract media type + keywords if provided)
2. **Guide** - If unclear, show crawler options using list_all_crawlers.py
3. **Ask** - If keyword missing, ask user directly (no dropdowns for "other")
4. **Validate** - Use validate_params.py strictly
5. **Confirm** - Always show confirmation dialog with AskUserQuestion
6. **Execute** - Use create_crawl_order.py (handles all steps)
7. **Notify** - Success/failure notifications via Feishu

**NEVER skip steps or write custom scripts. Always use pre-built scripts from scripts/ directory.**
