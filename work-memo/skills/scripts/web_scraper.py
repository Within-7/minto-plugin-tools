#!/usr/bin/env python3
"""
Web Content Scraper - æŠ“å–å¹¶æ€»ç»“ç½‘é¡µå†…å®¹

Usage:
    python web_scraper.py "https://example.com"
    python web_scraper.py "https://example.com" --tags work research
"""

import sys
import argparse
import re
from pathlib import Path
from typing import Optional, List
from urllib.parse import urlparse
import json

# å°è¯•å¯¼å…¥requestså’ŒBeautifulSoup
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… requests å’Œ beautifulsoup4")
    print("è¿è¡Œ: pip install requests beautifulsoup4")
    sys.exit(1)

# æ·»åŠ  scripts ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(SCRIPT_DIR))

from markdown_storage import MarkdownStorage
from ai_analyzer import AIAnalyzer
from schema import WorkRecord, WorkType, Status


def is_valid_url(url: str) -> bool:
    """éªŒè¯URLæ ¼å¼"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False


def fetch_webpage(url: str, timeout: int = 10) -> Optional[str]:
    """
    è·å–ç½‘é¡µå†…å®¹

    Args:
        url: ç½‘é¡µURL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        str: ç½‘é¡µHTMLå†…å®¹
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–ç½‘é¡µå¤±è´¥: {e}")
        return None


def extract_content(html: str) -> dict:
    """
    ä»HTMLä¸­æå–ä¸»è¦å†…å®¹

    Args:
        html: HTMLå†…å®¹

    Returns:
        dict: åŒ…å«title, content, summaryçš„å­—å…¸
    """
    soup = BeautifulSoup(html, 'html.parser')

    # æå–æ ‡é¢˜
    title = ""
    if soup.title:
        title = soup.title.get_text().strip()
    if not title:
        title_tag = soup.find('h1')
        if title_tag:
            title = title_tag.get_text().strip()

    # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
        tag.decompose()

    # æå–ä¸»è¦å†…å®¹
    content = ""
    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|article|post|entry'))
    
    if main_content:
        # æå–æ®µè½
        paragraphs = main_content.find_all('p')
        content = '\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
    else:
        # å›é€€åˆ°æ‰€æœ‰æ®µè½
        paragraphs = soup.find_all('p')
        content = '\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

    # æ¸…ç†å†…å®¹
    content = re.sub(r'\s+', ' ', content)
    content = content.strip()

    # ç”Ÿæˆæ‘˜è¦ï¼ˆå‰500å­—ç¬¦ï¼‰
    summary = content[:500] + "..." if len(content) > 500 else content

    return {
        'title': title,
        'content': content,
        'summary': summary
    }


def summarize_content(content: str, max_length: int = 300) -> str:
    """
    æ€»ç»“å†…å®¹ï¼ˆç®€å•ç‰ˆæœ¬ï¼Œå®é™…å¯ä»¥ä½¿ç”¨AIæ¨¡å‹ï¼‰

    Args:
        content: åŸå§‹å†…å®¹
        max_length: æœ€å¤§é•¿åº¦

    Returns:
        str: æ€»ç»“å†…å®¹
    """
    # ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼šæå–å…³é”®å¥å­
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # è¿”å›å‰å‡ ä¸ªå¥å­
    summary = '. '.join(sentences[:3])
    
    if len(summary) > max_length:
        summary = summary[:max_length] + "..."
    
    return summary


def record_webpage(url: str, tags: Optional[List[str]] = None, urgency: int = 3, importance: int = 3) -> dict:
    """
    æŠ“å–ç½‘é¡µå†…å®¹å¹¶è®°å½•åˆ°å¤‡å¿˜å½•ï¼ˆä½¿ç”¨ Markdown å­˜å‚¨ï¼‰

    Args:
        url: ç½‘é¡µURL
        tags: æ ‡ç­¾åˆ—è¡¨
        urgency: ç´§æ€¥ç¨‹åº¦ (1-5)
        importance: é‡è¦ç¨‹åº¦ (1-5)

    Returns:
        dict: è®°å½•ä¿¡æ¯
    """
    # éªŒè¯URL
    if not is_valid_url(url):
        return {
            'status': 'error',
            'message': 'æ— æ•ˆçš„URLæ ¼å¼'
        }

    # è·å–ç½‘é¡µå†…å®¹
    html = fetch_webpage(url)
    if not html:
        return {
            'status': 'error',
            'message': 'æ— æ³•è·å–ç½‘é¡µå†…å®¹'
        }

    # æå–å†…å®¹
    extracted = extract_content(html)

    if not extracted['title'] and not extracted['content']:
        return {
            'status': 'error',
            'message': 'æ— æ³•æå–ç½‘é¡µå†…å®¹'
        }

    # ç”Ÿæˆæ‘˜è¦
    summary = summarize_content(extracted['content'])

    # ä½¿ç”¨ Markdown å­˜å‚¨
    storage = MarkdownStorage()
    analyzer = AIAnalyzer()

    # æ„å»ºæ ‡é¢˜
    title = extracted['title'] if extracted['title'] else "ç½‘é¡µå†…å®¹"
    description = f"æ¥æº: {url}\n\næ‘˜è¦: {summary}"

    # åˆ›å»ºè®°å½•
    record = WorkRecord(
        title=title,
        description=description,
        type=WorkType.RESEARCH,  # é»˜è®¤ä¸ºç ”ç©¶ç±»å‹
        status=Status.TODO,
        urgency=urgency,
        importance=importance,
        difficulty=5,
    )

    # æ·»åŠ é»˜è®¤æ ‡ç­¾
    final_tags = ['web', 'reading']
    if tags:
        final_tags.extend(tags)
    record.tags = list(set(final_tags))

    # æ·»åŠ URLåˆ°ä¸Šä¸‹æ–‡
    record.contexts = [url]

    # AI åˆ†æ
    original_input = f"ç½‘é¡µå†…å®¹: {url}"
    ai_analysis = analyzer.analyze(original_input)

    # ä¿å­˜åˆ° Markdown å­˜å‚¨
    record_id = storage.create(
        record=record,
        original_input=original_input,
        ai_analysis=ai_analysis
    )

    # è¿”å›è®°å½•ä¿¡æ¯
    result = {
        'status': 'success',
        'id': record_id,
        'title': record.title,
        'url': url,
        'summary': summary,
        'type': record.type.value,
        'urgency': record.urgency,
        'importance': record.importance,
        'tags': record.tags,
        'contexts': record.contexts,
        'eisenhower': record.get_eisenhower_quadrant(),
        'created_at': record.created_at,
    }

    return result


def print_webpage_record(record: dict):
    """æ ¼å¼åŒ–æ‰“å°ç½‘é¡µè®°å½•"""
    if record['status'] == 'error':
        print(f"âŒ {record['message']}")
        return

    print(f"ğŸ“ å·²è®°å½•: {record['id']}")
    print(f"   æ ‡é¢˜: {record['title']}")
    print(f"   URL: {record['url']}")
    print(f"   æ‘˜è¦: {record['summary']}")
    print(f"   ç±»å‹: {record['type']}")
    print(f"   ä¼˜å…ˆçº§: ç´§æ€¥åº¦ {record['urgency']}/5, é‡è¦åº¦ {record['importance']}/5")
    print(f"   æ ‡ç­¾: {', '.join(record['tags'])}")
    print(f"   Eisenhower: {record['eisenhower']}")
    print(f"   åˆ›å»ºæ—¶é—´: {record['created_at']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æŠ“å–ç½‘é¡µå†…å®¹å¹¶è®°å½•åˆ°å¤‡å¿˜å½•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŠ“å–ç½‘é¡µå¹¶è®°å½•
  python web_scraper.py "https://example.com/article"
  
  # æŠ“å–ç½‘é¡µå¹¶æ·»åŠ æ ‡ç­¾
  python web_scraper.py "https://example.com/article" --tags work research
  
  # è®¾ç½®ä¼˜å…ˆçº§
  python web_scraper.py "https://example.com/article" --urgency 4 --importance 5
        """
    )

    parser.add_argument(
        'url',
        help='ç½‘é¡µURL'
    )
    parser.add_argument(
        '--tags',
        nargs='+',
        help='æ ‡ç­¾åˆ—è¡¨'
    )
    parser.add_argument(
        '--urgency',
        type=int,
        choices=range(1, 6),
        default=3,
        help='ç´§æ€¥ç¨‹åº¦ (1-5, 5=æœ€ç´§æ€¥)'
    )
    parser.add_argument(
        '--importance',
        type=int,
        choices=range(1, 6),
        default=3,
        help='é‡è¦ç¨‹åº¦ (1-5, 5=æœ€é‡è¦)'
    )

    args = parser.parse_args()

    # æ‰§è¡ŒæŠ“å–å’Œè®°å½•
    result = record_webpage(
        url=args.url,
        tags=args.tags,
        urgency=args.urgency,
        importance=args.importance,
    )
    
    print_webpage_record(result)
    
    if result['status'] == 'success':
        print("\nâœ… ç½‘é¡µå†…å®¹å·²æŠ“å–å¹¶è®°å½•")


if __name__ == '__main__':
    main()
